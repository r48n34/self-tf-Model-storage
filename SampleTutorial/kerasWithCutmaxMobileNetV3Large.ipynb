{
  "nbformat": 4,
  "nbformat_minor": 2,
  "metadata": {
    "colab": {
      "name": "kerasWithCutmaxMobileNetV3Large.ipynb",
      "provenance": [],
      "collapsed_sections": []
    },
    "kernelspec": {
      "name": "python3",
      "display_name": "Python 3"
    },
    "language_info": {
      "name": "python"
    },
    "accelerator": "GPU"
  },
  "cells": [
    {
      "cell_type": "code",
      "execution_count": null,
      "source": [
        "gpu_info = !nvidia-smi\r\n",
        "gpu_info = '\\n'.join(gpu_info)\r\n",
        "if gpu_info.find('failed') >= 0:\r\n",
        "  print('Not connected to a GPU')\r\n",
        "else:\r\n",
        "  print(gpu_info)"
      ],
      "outputs": [],
      "metadata": {
        "id": "ts6MqPaXwEoK"
      }
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "source": [
        "from google.colab import drive\r\n",
        "drive.mount('/content/drive')"
      ],
      "outputs": [],
      "metadata": {
        "id": "dmrTeXOfwDct"
      }
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "source": [
        "from numpy.core.numeric import False_\r\n",
        "import tensorflow as tf\r\n",
        "from tensorflow.keras import datasets, layers, models\r\n",
        "import matplotlib.pyplot as plt\r\n",
        "import numpy as np\r\n",
        "from datetime import datetime\r\n",
        "\r\n",
        "\r\n",
        "import matplotlib.pyplot as plt\r\n",
        "from tensorflow import keras\r\n",
        "from tensorflow.keras.models import Sequential\r\n",
        "\r\n",
        "np.random.seed(42)\r\n",
        "tf.random.set_seed(42)"
      ],
      "outputs": [],
      "metadata": {
        "id": "EVtmxawmsTTs"
      }
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "source": [
        "data_dir = \"/content/drive/MyDrive/food\"\r\n",
        "\r\n",
        "batch_size = 64\r\n",
        "imgSize = 224\r\n",
        "\r\n",
        "train_ds = tf.keras.preprocessing.image_dataset_from_directory(\r\n",
        "  data_dir, seed=123, subset=\"training\", validation_split=0.2,\r\n",
        "  image_size=(imgSize, imgSize), batch_size=batch_size\r\n",
        ")\r\n",
        "\r\n",
        "valid_ds = tf.keras.preprocessing.image_dataset_from_directory(\r\n",
        "  data_dir, seed=123, subset=\"validation\", validation_split=0.2,\r\n",
        "  image_size=(imgSize, imgSize), batch_size=batch_size\r\n",
        ")\r\n",
        "\r\n",
        "classNum = len(train_ds.class_names)\r\n",
        "labArr = train_ds.class_names\r\n",
        "print(train_ds.class_names)"
      ],
      "outputs": [],
      "metadata": {
        "id": "VZ05uT3osj2k"
      }
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "source": [
        "# Data expend for image augmentation\r\n",
        "expendRound = 1\r\n",
        "temp_ds = train_ds\r\n",
        "for i in range(expendRound):\r\n",
        "    train_ds = train_ds.concatenate(temp_ds)\r\n",
        "\r\n",
        "train_ds_Conbine = tf.data.Dataset.zip((train_ds, train_ds))"
      ],
      "outputs": [],
      "metadata": {
        "id": "LSzgRMxKsgML"
      }
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "source": [
        "# Cutmix distribution\r\n",
        "def sample_beta_distribution(size, concentration_0=0.2, concentration_1=0.2):\r\n",
        "    gamma_1_sample = tf.random.gamma(shape=[size], alpha=concentration_1)\r\n",
        "    gamma_2_sample = tf.random.gamma(shape=[size], alpha=concentration_0)\r\n",
        "    return gamma_1_sample / (gamma_1_sample + gamma_2_sample)\r\n",
        "\r\n",
        "# Cutmix box generator\r\n",
        "def get_box(lambda_value):\r\n",
        "    cut_rat = tf.math.sqrt(1.0 - lambda_value)\r\n",
        "\r\n",
        "    cut_w = imgSize * cut_rat  # rw\r\n",
        "    cut_w = tf.cast(cut_w, tf.int32)\r\n",
        "\r\n",
        "    cut_h = imgSize * cut_rat  # rh\r\n",
        "    cut_h = tf.cast(cut_h, tf.int32)\r\n",
        "\r\n",
        "    cut_x = tf.random.uniform((1,), minval=0, maxval=imgSize, dtype=tf.int32)  # rx\r\n",
        "    cut_y = tf.random.uniform((1,), minval=0, maxval=imgSize, dtype=tf.int32)  # ry\r\n",
        "\r\n",
        "    boundaryx1 = tf.clip_by_value(cut_x[0] - cut_w // 2, 0, imgSize)\r\n",
        "    boundaryy1 = tf.clip_by_value(cut_y[0] - cut_h // 2, 0, imgSize)\r\n",
        "    bbx2 = tf.clip_by_value(cut_x[0] + cut_w // 2, 0, imgSize)\r\n",
        "    bby2 = tf.clip_by_value(cut_y[0] + cut_h // 2, 0, imgSize)\r\n",
        "\r\n",
        "    target_h = bby2 - boundaryy1\r\n",
        "    if target_h == 0:\r\n",
        "        target_h += 1\r\n",
        "\r\n",
        "    target_w = bbx2 - boundaryx1\r\n",
        "    if target_w == 0:\r\n",
        "        target_w += 1\r\n",
        "\r\n",
        "    return boundaryx1, boundaryy1, target_h, target_w\r\n",
        "\r\n",
        "def cutmix(train_ds_one, train_ds_two):\r\n",
        "    (image1, label1), (image2, label2) = train_ds_one, train_ds_two\r\n",
        "\r\n",
        "    alpha = [0.25]\r\n",
        "    beta = [0.25]\r\n",
        "\r\n",
        "    # Get a sample from the Beta distribution\r\n",
        "    lambda_value = sample_beta_distribution(1, alpha, beta)\r\n",
        "\r\n",
        "    # Define Lambda\r\n",
        "    lambda_value = lambda_value[0][0]\r\n",
        "\r\n",
        "    # Get the bounding box offsets, heights and widths\r\n",
        "    boundaryx1, boundaryy1, target_h, target_w = get_box(lambda_value)\r\n",
        "\r\n",
        "    # Get a patch from the second image (`image2`)\r\n",
        "    crop2 = tf.image.crop_to_bounding_box(\r\n",
        "        image2, boundaryy1, boundaryx1, target_h, target_w\r\n",
        "    )\r\n",
        "    # Pad the `image2` patch (`crop2`) with the same offset\r\n",
        "    image2 = tf.image.pad_to_bounding_box(\r\n",
        "        crop2, boundaryy1, boundaryx1, imgSize, imgSize\r\n",
        "    )\r\n",
        "    # Get a patch from the first image (`image1`)\r\n",
        "    crop1 = tf.image.crop_to_bounding_box(\r\n",
        "        image1, boundaryy1, boundaryx1, target_h, target_w\r\n",
        "    )\r\n",
        "    # Pad the `image1` patch (`crop1`) with the same offset\r\n",
        "    img1 = tf.image.pad_to_bounding_box(\r\n",
        "        crop1, boundaryy1, boundaryx1, imgSize, imgSize\r\n",
        "    )\r\n",
        "\r\n",
        "    # Modify the first image by subtracting the patch from `image1`\r\n",
        "    # (before applying the `image2` patch)\r\n",
        "    image1 = image1 - img1\r\n",
        "    # Add the modified `image1` and `image2`  together to get the CutMix image\r\n",
        "    image = image1 + image2\r\n",
        "\r\n",
        "    # Adjust Lambda in accordance to the pixel ration\r\n",
        "    lambda_value = 1 - (target_w * target_h) / (imgSize * imgSize)\r\n",
        "    lambda_value = tf.cast(lambda_value, tf.float32)\r\n",
        "\r\n",
        "    # Combine the labels of both images\r\n",
        "    #label = lambda_value * label1 + (1 - lambda_value) * label2\r\n",
        "\r\n",
        "    return image, label1"
      ],
      "outputs": [],
      "metadata": {
        "id": "8U3QxdUItIgZ"
      }
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "source": [
        "# Apply cutmax images to train_ds_cmu\r\n",
        "train_ds_cmu = train_ds_Conbine.map(cutmix)\r\n",
        "\r\n",
        "data_augmentation = keras.Sequential(\r\n",
        "  [\r\n",
        "    layers.experimental.preprocessing.RandomFlip(\"horizontal_and_vertical\"),\r\n",
        "    layers.experimental.preprocessing.RandomRotation(0.1),\r\n",
        "    layers.experimental.preprocessing.RandomZoom(0.1),\r\n",
        "    layers.experimental.preprocessing.RandomContrast(0.1),\r\n",
        "  ]\r\n",
        ")\r\n",
        "# Enable below if doing data_augmentation\r\n",
        "#train_ds_cmu = train_ds_cmu.map(lambda image,label:(data_augmentation(image),label))"
      ],
      "outputs": [],
      "metadata": {
        "id": "RQduFS_etMhV"
      }
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "source": [
        "# plot images\r\n",
        "plt.figure(figsize=(10, 10))\r\n",
        "for images, labels in train_ds_cmu.take(1):\r\n",
        "  #print(labels)\r\n",
        "  labelArr = labels.numpy()\r\n",
        "  for i in range(12):\r\n",
        "    ax = plt.subplot(3, 4, i + 1)\r\n",
        "    plt.imshow(images[i].numpy().astype(\"uint8\"))\r\n",
        "    plt.title( labArr[labelArr[i]] )\r\n",
        "    plt.axis(\"off\")\r\n",
        "\r\n",
        "plt.show()"
      ],
      "outputs": [],
      "metadata": {
        "id": "9danc916t1J2"
      }
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "source": [
        "baseModel = tf.keras.applications.MobileNetV3Large(input_shape=(imgSize,imgSize,3),\r\n",
        "                          include_top=False,\r\n",
        "                          weights='imagenet')\r\n",
        "\r\n",
        "baseModel.trainable = True\r\n",
        "print(\"Layers count\", len(baseModel.layers))\r\n",
        "\r\n",
        "# fine tune numbers\r\n",
        "fine_tune_at = int( len(baseModel.layers) * 0.6)\r\n",
        "for layer in baseModel.layers[:fine_tune_at]:\r\n",
        "  layer.trainable = False\r\n",
        "\r\n",
        "model = Sequential([\r\n",
        "  baseModel,\r\n",
        "  tf.keras.layers.GlobalAveragePooling2D(),\r\n",
        "  tf.keras.layers.Dropout(0.1),\r\n",
        "  tf.keras.layers.Dense(classNum, activation=tf.nn.softmax)\r\n",
        "])"
      ],
      "outputs": [],
      "metadata": {
        "id": "_2s9uVAZt2eA"
      }
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "source": [
        "epochsRound = 12\r\n",
        "base_learning_rate = 0.0001\r\n",
        "\r\n",
        "checkpoint_filepath = './tmp/checkpoint'\r\n",
        "model_checkpoint_callback = tf.keras.callbacks.ModelCheckpoint(\r\n",
        "    filepath=checkpoint_filepath,\r\n",
        "    save_weights_only=True,\r\n",
        "    monitor='val_accuracy',\r\n",
        "    mode='max',\r\n",
        "    save_best_only=True\r\n",
        ")\r\n",
        "\r\n",
        "model.compile(optimizer=tf.keras.optimizers.Adam(lr=base_learning_rate),\r\n",
        "              loss=tf.keras.losses.SparseCategoricalCrossentropy(from_logits=True),\r\n",
        "              metrics=['accuracy'])\r\n",
        "\r\n",
        "history = model.fit(train_ds_cmu, epochs=epochsRound,\r\n",
        "          validation_data=valid_ds, callbacks=[model_checkpoint_callback])\r\n",
        "\r\n",
        "now = datetime.now()\r\n",
        "current_time = now.strftime(\"%b-%d-%Y_%H:%M:%S\")\r\n",
        "srt = \"/content/drive/MyDrive/savedModel/\" + \"efficientnetV1B0\" + current_time + \".h5\"\r\n",
        "\r\n",
        "model.load_weights(checkpoint_filepath)\r\n",
        "\r\n",
        "test_loss, test_acc = model.evaluate(valid_ds, verbose=2)\r\n",
        "print(test_acc)\r\n",
        "\r\n",
        "model.save(srt)"
      ],
      "outputs": [],
      "metadata": {
        "id": "VoyURhRzt_2_"
      }
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "source": [
        "acc = history.history['accuracy']\r\n",
        "val_acc = history.history['val_accuracy']\r\n",
        "loss = history.history['loss']\r\n",
        "val_loss = history.history['val_loss']\r\n",
        "\r\n",
        "epochs_range = range(epochsRound)\r\n",
        "\r\n",
        "plt.figure(figsize=(8, 8))\r\n",
        "plt.subplot(1, 2, 1)\r\n",
        "plt.plot(epochs_range, acc, label='Training Accuracy')\r\n",
        "plt.plot(epochs_range, val_acc, label='Validation Accuracy')\r\n",
        "plt.legend(loc='lower right')\r\n",
        "plt.title('Training and Validation Accuracy')\r\n",
        "\r\n",
        "plt.subplot(1, 2, 2)\r\n",
        "plt.plot(epochs_range, loss, label='Training Loss')\r\n",
        "plt.plot(epochs_range, val_loss, label='Validation Loss')\r\n",
        "plt.legend(loc='upper right')\r\n",
        "plt.title('Training and Validation Loss')\r\n",
        "plt.show()"
      ],
      "outputs": [],
      "metadata": {
        "id": "AyZt6VljuFqq"
      }
    },
    {
      "cell_type": "markdown",
      "source": [
        "# References  \n",
        "\n",
        "1. CutMix data augmentation for image classification:  \n",
        "https://keras.io/examples/vision/cutmix/  \n",
        "2. Get started with TensorBoard  \n",
        "https://www.tensorflow.org/tensorboard/get_started?hl=zh-tw  \n",
        "3. Transfer learning and fine-tuning  \n",
        "https://www.tensorflow.org/tutorials/images/transfer_learning?hl=zh-tw"
      ],
      "metadata": {
        "id": "w34lmUOAup43"
      }
    }
  ]
}