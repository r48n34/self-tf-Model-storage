{
  "nbformat": 4,
  "nbformat_minor": 2,
  "metadata": {
    "colab": {
      "name": "transferLearningWithKeras.ipynb",
      "provenance": [],
      "collapsed_sections": []
    },
    "kernelspec": {
      "name": "python3",
      "display_name": "Python 3"
    },
    "language_info": {
      "name": "python"
    }
  },
  "cells": [
    {
      "cell_type": "markdown",
      "source": [
        "# Introduce to deep learningin colab environment (runnable)  \n",
        "Last update: (10/9/2021)\n"
      ],
      "metadata": {
        "id": "1Ec089HD0Z5g"
      }
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "source": [
        "gpu_info = !nvidia-smi\r\n",
        "gpu_info = '\\n'.join(gpu_info)\r\n",
        "if gpu_info.find('failed') >= 0:\r\n",
        "  print('Not connected to a GPU')\r\n",
        "else:\r\n",
        "  print(gpu_info)"
      ],
      "outputs": [],
      "metadata": {
        "id": "M_gQXgnZ2VoG"
      }
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "source": [
        "from google.colab import drive\r\n",
        "drive.mount('/content/drive')"
      ],
      "outputs": [],
      "metadata": {
        "id": "L1L8lwTx2TO_"
      }
    },
    {
      "cell_type": "markdown",
      "source": [
        "## Import library\n"
      ],
      "metadata": {
        "id": "bQC_TXUJ0qdi"
      }
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "source": [
        "from numpy.core.numeric import False_\r\n",
        "import tensorflow as tf\r\n",
        "from tensorflow.keras import datasets, layers, models\r\n",
        "import matplotlib.pyplot as plt\r\n",
        "import numpy as np\r\n",
        "import os\r\n",
        "\r\n",
        "from tensorflow import keras\r\n",
        "from tensorflow.keras.models import Sequential"
      ],
      "outputs": [],
      "metadata": {
        "id": "WspCLVtFnEGx"
      }
    },
    {
      "cell_type": "markdown",
      "source": [
        "## Data import"
      ],
      "metadata": {
        "id": "d26_Gdx46wGx"
      }
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "source": [
        "# Set up your file path \r\n",
        "data_dir = \"/content/drive/MyDrive/food\"\r\n",
        "\r\n",
        "# Change you batch size here\r\n",
        "# In general, batch_size is depends to your GPU/CPU ability, strong GUP may set a larger number of batch size\r\n",
        "# From 2,4,8,16,32,64,128,256,512,1023,2048...\r\n",
        "batch_size = 32\r\n",
        "\r\n",
        "# Change you imgSize here\r\n",
        "# imgSize is depend on model needed. For example, mobienetV3Large accept (224,224,3) shape, so we set 224 here\r\n",
        "imgSize = 224\r\n",
        "\r\n",
        "# split you data into 80% (training) and 20% (validations) for 0.2\r\n",
        "dataSplitRate = 0.2\r\n",
        "\r\n",
        "# Set train dataset\r\n",
        "train_ds = tf.keras.preprocessing.image_dataset_from_directory(\r\n",
        "  data_dir, seed=123, subset=\"training\", validation_split=dataSplitRate,\r\n",
        "  image_size=(imgSize, imgSize), batch_size=batch_size\r\n",
        ")\r\n",
        "\r\n",
        "# Set valid dataset\r\n",
        "valid_ds = tf.keras.preprocessing.image_dataset_from_directory(\r\n",
        "  data_dir, seed=123, subset=\"validation\", validation_split=dataSplitRate,\r\n",
        "  image_size=(imgSize, imgSize), batch_size=batch_size\r\n",
        ")\r\n",
        "\r\n",
        "# print the class info with class array\r\n",
        "classNum = len(train_ds.class_names)\r\n",
        "print(train_ds.class_names)\r\n",
        "\r\n",
        "# plt the graph\r\n",
        "plt.figure(figsize=(10, 10))\r\n",
        "for images, labels in train_ds.take(1):\r\n",
        "  for i in range(9):\r\n",
        "    ax = plt.subplot(3, 3, i + 1)\r\n",
        "    plt.imshow(images[i].numpy().astype(\"uint8\"))\r\n",
        "    plt.title(train_ds.class_names[labels[i]])\r\n",
        "    plt.axis(\"off\")\r\n",
        "\r\n",
        "plt.show()"
      ],
      "outputs": [],
      "metadata": {
        "id": "sFPVSZzUnIBK"
      }
    },
    {
      "cell_type": "markdown",
      "source": [
        "## Data augmentation\n",
        "If you own data numbers is quite small, you may apply some more data from random adjust the attributes of the images.    \n",
        "See more ...   \n",
        "https://www.tensorflow.org/api_docs/python/tf/keras/layers/experimental/preprocessing"
      ],
      "metadata": {
        "id": "0Bc3sBMX2X0j"
      }
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "source": [
        "# method for data transfer, you may adjust here but we stack on this transfer first\r\n",
        "data_augmentation = keras.Sequential(\r\n",
        "  [\r\n",
        "    layers.experimental.preprocessing.RandomFlip(\"horizontal_and_vertical\"),\r\n",
        "    layers.experimental.preprocessing.RandomRotation(0.1),\r\n",
        "    layers.experimental.preprocessing.RandomZoom(0.1),\r\n",
        "  ]\r\n",
        ")\r\n",
        "\r\n",
        "# Data expend for image augmentation, AKA copy and paste your images by times\r\n",
        "# you may adjust the expendRound numbers here, 0 means no expend\r\n",
        "expendRound = 1\r\n",
        "temp_ds = train_ds\r\n",
        "for i in range(expendRound):\r\n",
        "    train_ds = train_ds.concatenate(temp_ds)\r\n",
        "\r\n",
        "# Apply the data_augmentation to dataset\r\n",
        "train_ds = train_ds.map(lambda image,label:(data_augmentation(image),label))"
      ],
      "outputs": [],
      "metadata": {
        "id": "uRPiUiQr2rt1"
      }
    },
    {
      "cell_type": "markdown",
      "source": [
        "#Resize and / or rescale\n",
        "Different model may apply to their data structure. For example,MobileNetV3Large accept the color range input [0,255] and (224,224,3) "
      ],
      "metadata": {
        "id": "O4cP8kK73Ldy"
      }
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "source": [
        "resize_and_rescale = tf.keras.Sequential([\r\n",
        "  layers.experimental.preprocessing.Resizing(imgSize, imgSize),\r\n",
        "  # In gereral, if the model require [-1,1] or [0,1] input, you may need to able the below line\r\n",
        "  # NO need to apply Rescaling if model apply [0,255]\r\n",
        "  #layers.experimental.preprocessing.Rescaling(1./127.5, offset=-1) # [-1,1]\r\n",
        "  #layers.experimental.preprocessing.Rescaling(1./255) # [0,1]\r\n",
        "])\r\n",
        "\r\n",
        "# Apply the resize_and_rescale to dataset\r\n",
        "train_ds = train_ds.map(lambda image,label:(resize_and_rescale(image),label))\r\n",
        "valid_ds = valid_ds.map(lambda image,label:(resize_and_rescale(image),label))"
      ],
      "outputs": [],
      "metadata": {
        "id": "KAtP68J34vUX"
      }
    },
    {
      "cell_type": "markdown",
      "source": [
        "**Cont' General references of which model need to be rescale or not**  \n",
        "To specific, 0-255 means the RGB range, not the image size!  (Update 2/9/2021)\n",
        " \n",
        "**[0,255]**  \n",
        "- EfficientNetB0-7  \n",
        "https://www.tensorflow.org/api_docs/python/tf/keras/applications/efficientnet/preprocess_input\n",
        "- MobileNetV3Large / MobileNetV3Small  \n",
        "https://www.tensorflow.org/api_docs/python/tf/keras/applications/mobilenet_v3/preprocess_input  \n",
        "- ResNet50 / ResNet101 / ResNet152 (RGB to BGR**)   \n",
        "https://www.tensorflow.org/api_docs/python/tf/keras/applications/resnet50/preprocess_input  \n",
        "- VGG16 (RGB to BGR**)   \n",
        "https://www.tensorflow.org/api_docs/python/tf/keras/applications/vgg16/preprocess_input  \n",
        "- VGG19 (RGB to BGR**)   \n",
        "https://www.tensorflow.org/api_docs/python/tf/keras/applications/vgg19/preprocess_input\n",
        "\n",
        "\n",
        "**[0,1]**  \n",
        "- DenseNet  \n",
        "https://www.tensorflow.org/api_docs/python/tf/keras/applications/densenet/preprocess_input  \n",
        "\n",
        "\n",
        "**[-1,1]**  \n",
        "- MobileNet  \n",
        "https://www.tensorflow.org/api_docs/python/tf/keras/applications/mobilenet/preprocess_input    \n",
        "- MobileNetV2  \n",
        "https://www.tensorflow.org/api_docs/python/tf/keras/applications/mobilenet_v2/preprocess_input  \n",
        "- NASNetLarge / NASNetMobile  \n",
        "https://www.tensorflow.org/api_docs/python/tf/keras/applications/nasnet/preprocess_input  \n",
        "- InceptionResNetV2  \n",
        "https://www.tensorflow.org/api_docs/python/tf/keras/applications/inception_resnet_v2/preprocess_input  \n",
        "- InceptionV3  \n",
        "https://www.tensorflow.org/api_docs/python/tf/keras/applications/inception_v3/preprocess_input  \n",
        "- ResNet101V2 / ResNet152V2 / ResNet50V2  \n",
        "https://www.tensorflow.org/api_docs/python/tf/keras/applications/resnet_v2/preprocess_input  \n",
        "- Xception  \n",
        "https://www.tensorflow.org/api_docs/python/tf/keras/applications/xception/preprocess_input  \n",
        "\n",
        "See more:  \n",
        "\n",
        "TensorFlow Doc: https://www.tensorflow.org/api_docs/python/tf/keras/applications  \n",
        "Keras Applications: https://keras.io/api/applications/  \n",
        "Model rank: https://paperswithcode.com/sota/image-classification-on-imagenet  \n"
      ],
      "metadata": {
        "id": "DX-ZaDTu55ci"
      }
    },
    {
      "cell_type": "markdown",
      "source": [
        "## Set up model\n",
        "you may select a new model here  \n",
        "In general, if we are using the model to learn out dataSet, we should set the include_top=False to apply our top layers\n",
        "You may select the model from:   \n",
        "TF: https://www.tensorflow.org/api_docs/python/tf/keras/applications  \n",
        "Keras: https://keras.io/api/applications/\n",
        "\n",
        "## How to select a good moden for your data\n",
        "In Case, you may see several attributes from keras applications (Parameters, top-1 acc, top-5 acc)  \n",
        "**Parameters**: In general, bigger params model trains a higher accuracy. But takes more computer resources to train or predict inupt data.  \n",
        "**Top-1, Top-5** : The accuracy of predicting a game rules call 'imagenet' with 14,197,122 images for classifications 1000 classes.  \n",
        "Top-1 acc means the first highest possibility label predicts correctly.  \n",
        "Top-5 means the first top 5 possibility included the actual label.\n",
        "\n",
        "For general use, mobienetV3Large, efficientNetB1, efficientNetB2 is good enough for training a light weight and decent accuracy model.  \n",
        "If your GPU is good enough / you have a lot of times, you may select a larger model like EffNetV2-M or EffV1B3 to train your data.  \n",
        "\n",
        "See more: https://github.com/leondgarse/keras_efficientnet_v2\n"
      ],
      "metadata": {
        "id": "FTlqfJl_16qx"
      }
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "source": [
        "baseModel = tf.keras.applications.MobileNetV3Large(input_shape=(imgSize,imgSize,3), include_top=False, weights='imagenet')"
      ],
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "Downloading data from https://storage.googleapis.com/tensorflow/keras-applications/mobilenet_v3/weights_mobilenet_v3_large_224_1.0_float_no_top.h5\n",
            "17612800/17605208 [==============================] - 0s 0us/step\n",
            "17620992/17605208 [==============================] - 0s 0us/step\n"
          ]
        }
      ],
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "LJnq_v7qxm6M",
        "outputId": "926d338b-cb9a-4ccb-8df8-1cd0738ee67d"
      }
    },
    {
      "cell_type": "markdown",
      "source": [
        "## Apply our top layer to model\n"
      ],
      "metadata": {
        "id": "tJ3k6dA-7pM_"
      }
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "source": [
        "model = Sequential([\r\n",
        "  baseModel,\r\n",
        "  tf.keras.layers.GlobalAveragePooling2D(),\r\n",
        "  tf.keras.layers.Dropout(0.1),\r\n",
        "  tf.keras.layers.Dense(classNum, activation=tf.nn.softmax)\r\n",
        "])"
      ],
      "outputs": [],
      "metadata": {
        "id": "mi9ATNkhxxK9"
      }
    },
    {
      "cell_type": "markdown",
      "source": [
        "## fine tune  \n",
        "You will see that in baseModel, we apply the weights='imagenet'.  \n",
        "It means that the model have already trained with imagenet game rules with tuned weights. For out own dataset, we need to apply our own dataset rather than original weights.  \n",
        "Hence, we may lock up the front layer for using original imagenet weights, then set the end layer with learnable weights.    \n",
        "Lockup layers may depends on your own dataset numbers and characteristics(features)."
      ],
      "metadata": {
        "id": "a2rv1_Qe6jYv"
      }
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "source": [
        "baseModel.trainable = True\r\n",
        "print(\"Layers count\", len(baseModel.layers))\r\n",
        "\r\n",
        "fine_tune_at = int( len(baseModel.layers) * 0.7)\r\n",
        "for layer in baseModel.layers[:fine_tune_at]:\r\n",
        "  layer.trainable = False"
      ],
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "Layers count 269\n"
          ]
        }
      ],
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "ubz4WCx5xvNH",
        "outputId": "333d3611-692e-4bb6-f105-c01819f18a6b"
      }
    },
    {
      "cell_type": "markdown",
      "source": [
        "## Model training\n",
        "**epochsRound** means rounds that the model will training   \n",
        "**base_learning_rate** means the rate to get to the min loss positions  \n",
        "Usually with 1, 0.1, 0.001, 0.0001, 0.00001\n"
      ],
      "metadata": {
        "id": "tWrI9slB89qk"
      }
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "source": [
        "epochsRound = 10\r\n",
        "\r\n",
        "checkpoint_filepath = './tmp/checkpoint'\r\n",
        "model_checkpoint_callback = tf.keras.callbacks.ModelCheckpoint(\r\n",
        "    filepath=checkpoint_filepath,\r\n",
        "    save_weights_only=True,\r\n",
        "    monitor='val_accuracy',\r\n",
        "    mode='max',\r\n",
        "    save_best_only=True\r\n",
        ")\r\n",
        "\r\n",
        "base_learning_rate = 0.0001\r\n",
        "model.compile(optimizer=tf.keras.optimizers.Adam(lr=base_learning_rate),\r\n",
        "              loss=tf.keras.losses.SparseCategoricalCrossentropy(from_logits=False),\r\n",
        "              metrics=['accuracy'])\r\n",
        "\r\n",
        "history = model.fit(train_ds, epochs=epochsRound,validation_data=valid_ds, callbacks=[model_checkpoint_callback] )\r\n",
        "\r\n",
        "model.load_weights(checkpoint_filepath)\r\n",
        "\r\n",
        "test_loss, test_acc = model.evaluate(valid_ds, verbose=2)\r\n",
        "print(test_acc)\r\n",
        "\r\n",
        "model.save('yourFirstModel.h5')"
      ],
      "outputs": [],
      "metadata": {
        "id": "bWGKqKvYxzOi"
      }
    },
    {
      "cell_type": "markdown",
      "source": [
        "## See result"
      ],
      "metadata": {
        "id": "YbFEDwZ3-PLK"
      }
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "source": [
        "acc = history.history['accuracy']\r\n",
        "val_acc = history.history['val_accuracy']\r\n",
        "loss = history.history['loss']\r\n",
        "val_loss = history.history['val_loss']\r\n",
        "\r\n",
        "epochs_range = range(epochsRound)\r\n",
        "\r\n",
        "plt.figure(figsize=(8, 8))\r\n",
        "plt.subplot(1, 2, 1)\r\n",
        "plt.plot(epochs_range, acc, label='Training Accuracy')\r\n",
        "plt.plot(epochs_range, val_acc, label='Validation Accuracy')\r\n",
        "plt.legend(loc='lower right')\r\n",
        "plt.title('Training and Validation Accuracy')\r\n",
        "\r\n",
        "plt.subplot(1, 2, 2)\r\n",
        "plt.plot(epochs_range, loss, label='Training Loss')\r\n",
        "plt.plot(epochs_range, val_loss, label='Validation Loss')\r\n",
        "plt.legend(loc='upper right')\r\n",
        "plt.title('Training and Validation Loss')\r\n",
        "plt.show()"
      ],
      "outputs": [],
      "metadata": {
        "id": "CFHqrs0yx5x3"
      }
    },
    {
      "cell_type": "markdown",
      "source": [
        "# Other Section / How to use your model with sanic"
      ],
      "metadata": {
        "id": "zEZh5jbnyYhc"
      }
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "source": [
        "# Sanic won't work on colab, but flask can run in colab.\r\n",
        "# Don't run this on colab environment\r\n",
        "from sanic import Sanic\r\n",
        "from sanic.response import json, text\r\n",
        "from sanic import response\r\n",
        "import os\r\n",
        "import aiofiles\r\n",
        "\r\n",
        "from keras.models import load_model\r\n",
        "from PIL import Image, ImageOps\r\n",
        "import numpy as np\r\n",
        "import os\r\n",
        "import tensorflow as tf\r\n",
        "\r\n",
        "app = Sanic(\"App Name\")\r\n",
        "# upload images will be downloaded to your server\r\n",
        "app.config[\"upload\"] = 'D:\\\\gh code\\\\codeNotes\\\\mlorDl\\\\sanicWebServices\\\\uploads'\r\n",
        "\r\n",
        "imgSize = 224\r\n",
        "\r\n",
        "label = [\r\n",
        "'Apple pie','Baby back ribs','Baklava','Beef carpaccio','Beef tartare','Beet salad',\r\n",
        "'Beignets','Bibimbap','Bread pudding','Breakfast burrito','Bruschetta','Caesar salad',\r\n",
        "'Cannoli','Caprese salad','Carrot cake','Ceviche','Cheesecake','Cheese plate',\r\n",
        "'Chicken curry','Chicken quesadilla','Chicken wings','Chocolate cake','Chocolate mousse','Churros',\r\n",
        "'Clam chowder','Club sandwich','Crab cakes','Creme brulee','Croque madame',\r\n",
        "'Cup cakes','Deviled eggs','Donuts','Dumplings','Edamame','Eggs benedict',\r\n",
        "'Escargots','Falafel','Filet mignon','Fish and chips','Foie gras','French fries',\r\n",
        "'French onion soup','French toast','Fried calamari','Fried rice','Frozen yogurt',\r\n",
        "'Garlic bread','Gnocchi','Greek salad','Grilled cheese sandwich','Grilled salmon',\r\n",
        "'Guacamole','Gyoza','Hamburger','Hot and sour soup','Hot dog','Huevos rancheros','Hummus',\r\n",
        "'Ice cream','Lasagna','Lobster bisque','Lobster roll sandwich','Macaroni and cheese','Macarons',\r\n",
        "'Miso soup','Mussels','Nachos','Omelette','Onion rings','Oysters','Pad thai','Paella','Pancakes','Panna cotta',\r\n",
        "'Peking duck','Pho','Pizza','Pork chop','Poutine','Prime rib','Pulled pork sandwich','Ramen','Ravioli','Red velvet cake',\r\n",
        "'Risotto','Samosa','Sashimi','Scallops','Seaweed salad','Shrimp and grits','Spaghetti bolognese','Spaghetti carbonara',\r\n",
        "'Spring rolls','Steak','Strawberry shortcake','Sushi','Tacos','Takoyaki','Tiramisu','Tuna tartare','Waffles'\r\n",
        "]\r\n",
        "\r\n",
        "model = load_model(\"D:\\\\efficientnetV1B1Food101.h5\") # Model path\r\n",
        "\r\n",
        "if not os.path.exists(app.config[\"upload\"]):\r\n",
        "    os.makedirs(app.config[\"upload\"])\r\n",
        "\r\n",
        "@app.get(\"/\")\r\n",
        "async def test(request):\r\n",
        "    return json({\"hello\": \"Ha\"})\r\n",
        "\r\n",
        "def getresult(imgName):\r\n",
        "    imgPath = os.path.join(app.config[\"upload\"], imgName)\r\n",
        "\r\n",
        "    image = Image.open(imgPath)\r\n",
        "    image = ImageOps.fit(image, (imgSize, imgSize) , Image.ANTIALIAS)\r\n",
        "\r\n",
        "    data = np.ndarray(shape=(1, imgSize, imgSize, 3), dtype=np.float32)\r\n",
        "    data[0] = (np.asarray(image).astype(np.float32) / 127.0) - 1 # Mobienetv2 rescale needs to be / 127.0 and -1 => outcome should be between [-1 to 1]\r\n",
        "    #data[0] = np.asarray(image).astype(np.float32)  # Model no need to be rescale => outcome should be between [0 to 255]\r\n",
        "\r\n",
        "    prediction = model.predict(data)\r\n",
        "    print(label[np.argmax(prediction)])\r\n",
        "    return prediction\r\n",
        "\r\n",
        "# The upload route with POST method\r\n",
        "@app.route(\"/upload\", methods=['POST'])\r\n",
        "async def getFiles(request):\r\n",
        "    fileName = request.files[\"file\"][0].name\r\n",
        "\r\n",
        "    async with aiofiles.open(app.config[\"upload\"] + \"/\" + fileName, 'wb' ) as f:\r\n",
        "        await f.write(request.files[\"file\"][0].body)\r\n",
        "    f.close()\r\n",
        "\r\n",
        "    prediction = getresult(fileName)\r\n",
        "    return text(label[np.argmax(prediction)])\r\n",
        "\r\n",
        "\r\n",
        "if __name__ == \"__main__\":\r\n",
        "    app.run(host=\"0.0.0.0\", port=8000)\r\n",
        "\r\n"
      ],
      "outputs": [],
      "metadata": {
        "id": "mXpcB6HwyiWB"
      }
    }
  ]
}
