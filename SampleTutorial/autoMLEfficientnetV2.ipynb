{
  "nbformat": 4,
  "nbformat_minor": 2,
  "metadata": {
    "colab": {
      "name": "autoMLEfficientnetV2.ipynb",
      "provenance": [],
      "collapsed_sections": []
    },
    "kernelspec": {
      "name": "python3",
      "display_name": "Python 3"
    },
    "language_info": {
      "name": "python"
    },
    "accelerator": "GPU"
  },
  "cells": [
    {
      "cell_type": "code",
      "execution_count": null,
      "source": [
        "from google.colab import drive\r\n",
        "drive.mount('/content/drive')"
      ],
      "outputs": [],
      "metadata": {
        "id": "ihawa4iM6OGc"
      }
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "source": [
        "gpu_info = !nvidia-smi\r\n",
        "gpu_info = '\\n'.join(gpu_info)\r\n",
        "if gpu_info.find('failed') >= 0:\r\n",
        "  print('Not connected to a GPU')\r\n",
        "else:\r\n",
        "  print(gpu_info)"
      ],
      "outputs": [],
      "metadata": {
        "id": "vrGvIkPa6W2s"
      }
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "source": [
        "# https://colab.research.google.com/github/google/automl/blob/master/efficientnetv2/tfhub.ipynb download code\r\n",
        "!pip install tensorflow_addons\r\n",
        "\r\n",
        "import os\r\n",
        "import sys\r\n",
        "import tensorflow.compat.v1 as tf\r\n",
        "\r\n",
        "if \"efficientnetv2\" not in os.getcwd():\r\n",
        "  !git clone --depth 1 https://github.com/google/automl\r\n",
        "  os.chdir('automl/efficientnetv2')\r\n",
        "  sys.path.append('.')\r\n",
        "else:\r\n",
        "  !git pull\r\n",
        "\r\n",
        "# download checkpoints ckpt\r\n",
        "def download(m):\r\n",
        "  if m not in os.listdir():\r\n",
        "    !wget https://storage.googleapis.com/cloud-tpu-checkpoints/efficientnet/v2/{m}.tgz\r\n",
        "    !tar zxf {m}.tgz\r\n",
        "  ckpt_path = os.path.join(os.getcwd(), m)\r\n",
        "  return ckpt_path\r\n",
        "\r\n",
        "# https://colab.research.google.com/github/google/automl/blob/master/efficientnetv2/tfhub.ipynb\r\n",
        "MODEL = 'efficientnetv2-b0'\r\n",
        "import effnetv2_model\r\n",
        "\r\n",
        "ckpt_path = download(MODEL)\r\n",
        "if tf.io.gfile.isdir(ckpt_path):\r\n",
        "  ckpt_path = tf.train.latest_checkpoint(ckpt_path)"
      ],
      "outputs": [],
      "metadata": {
        "id": "KeBEaAytgkdq"
      }
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "source": [
        "#import keras_efficientnet_v2\r\n",
        "import tensorflow as tf\r\n",
        "from tensorflow import keras\r\n",
        "import tensorflow_datasets as tfds\r\n",
        "\r\n",
        "from tensorflow.keras import datasets, layers, models\r\n",
        "import matplotlib.pyplot as plt\r\n",
        "import numpy as np\r\n",
        "from datetime import datetime"
      ],
      "outputs": [],
      "metadata": {
        "id": "H8TrRhDn6X7s"
      }
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "source": [
        "data_dir = \"/content/drive/MyDrive/food\"\r\n",
        "\r\n",
        "batch_size = 128\r\n",
        "imgSize = 224\r\n",
        "\r\n",
        "splitRate = 0.2\r\n",
        "\r\n",
        "train_ds = tf.keras.preprocessing.image_dataset_from_directory(\r\n",
        "  data_dir, seed=123, subset=\"training\", validation_split=splitRate,\r\n",
        "  image_size=(imgSize, imgSize), batch_size=batch_size\r\n",
        ")\r\n",
        "\r\n",
        "valid_ds = tf.keras.preprocessing.image_dataset_from_directory(\r\n",
        "  data_dir, seed=123, subset=\"validation\", validation_split=splitRate,\r\n",
        "  image_size=(imgSize, imgSize), batch_size=batch_size\r\n",
        ")\r\n",
        "\r\n",
        "classNum = len(train_ds.class_names)\r\n",
        "print(train_ds.class_names)"
      ],
      "outputs": [],
      "metadata": {
        "id": "zZxRr6056ZTX"
      }
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "source": [
        "data_augmentation = keras.Sequential(\r\n",
        "  [\r\n",
        "    layers.experimental.preprocessing.RandomFlip(\"horizontal_and_vertical\"),\r\n",
        "    layers.experimental.preprocessing.RandomRotation(0.1),\r\n",
        "    layers.experimental.preprocessing.RandomZoom(0.1),\r\n",
        "    layers.experimental.preprocessing.RandomContrast(0.1),\r\n",
        "\r\n",
        "  ]\r\n",
        ")\r\n",
        "\r\n",
        "resize_and_rescale = tf.keras.Sequential([\r\n",
        "  layers.experimental.preprocessing.Resizing(imgSize, imgSize),\r\n",
        "  layers.experimental.preprocessing.Rescaling(1./127.5, offset=-1)\r\n",
        "])"
      ],
      "outputs": [],
      "metadata": {
        "id": "kmHQIn2t6b2_"
      }
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "source": [
        "expendRound = 3\r\n",
        "temp_ds = train_ds\r\n",
        "for i in range(expendRound):\r\n",
        "    train_ds = train_ds.concatenate(temp_ds)\r\n",
        "\r\n",
        "train_ds = train_ds.map(lambda image,label:(data_augmentation(image),label))\r\n",
        "train_ds = train_ds.map(lambda image,label:(resize_and_rescale(image),label))\r\n",
        "\r\n",
        "valid_ds = valid_ds.map(lambda image,label:(resize_and_rescale(image),label))"
      ],
      "outputs": [],
      "metadata": {
        "id": "AkCf1D726dFG"
      }
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "source": [
        "# If you found that baseModel occur error, try to reset all cells and re-run\r\n",
        "baseModel = effnetv2_model.get_model(MODEL, include_top=False)\r\n",
        "\r\n",
        "baseModel.trainable = True\r\n",
        "print(\"Layers count\", len(baseModel.layers))\r\n",
        "\r\n",
        "fine_tune_at = int( len(baseModel.layers) * 0.2) # 20\r\n",
        "for layer in baseModel.layers[:fine_tune_at]:\r\n",
        "  layer.trainable = False\r\n",
        "\r\n",
        "model = tf.keras.models.Sequential([\r\n",
        "  tf.keras.layers.InputLayer(input_shape=[imgSize, imgSize, 3]),\r\n",
        "  baseModel,\r\n",
        "  tf.keras.layers.Dropout(rate=0.2),\r\n",
        "  tf.keras.layers.Dense(classNum, activation='softmax'),\r\n",
        "])\r\n",
        "  \r\n",
        "epochsRound = 9\r\n",
        "base_learning_rate = 0.0001\r\n",
        "\r\n",
        "checkpoint_filepath = './tmp/checkpoint'\r\n",
        "model_checkpoint_callback = tf.keras.callbacks.ModelCheckpoint(\r\n",
        "    filepath=checkpoint_filepath,\r\n",
        "    save_weights_only=True,\r\n",
        "    monitor='val_accuracy',\r\n",
        "    mode='max',\r\n",
        "    save_best_only=True\r\n",
        ")\r\n",
        "\r\n",
        "model.compile(optimizer=tf.keras.optimizers.Adam(lr=base_learning_rate),\r\n",
        "              loss=tf.keras.losses.SparseCategoricalCrossentropy(from_logits=True),\r\n",
        "              metrics=['accuracy'])\r\n",
        "\r\n",
        "history = model.fit(train_ds, epochs=epochsRound, validation_data=valid_ds, callbacks=[model_checkpoint_callback] )\r\n",
        "\r\n",
        "now = datetime.now()\r\n",
        "current_time = now.strftime(\"%b-%d-%Y_%H:%M:%S\")\r\n",
        "srt = \"/content/drive/MyDrive/savedModel/\" + \"efficientnetV1B0\" + current_time + \".h5\"\r\n",
        "\r\n",
        "model.load_weights(checkpoint_filepath)\r\n",
        "\r\n",
        "test_loss, test_acc = model.evaluate(valid_ds, verbose=2)\r\n",
        "print(test_acc)\r\n",
        "\r\n",
        "model.save(srt)"
      ],
      "outputs": [],
      "metadata": {
        "id": "8njvyPhk6gK3"
      }
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "source": [
        "acc = history.history['accuracy']\r\n",
        "val_acc = history.history['val_accuracy']\r\n",
        "loss = history.history['loss']\r\n",
        "val_loss = history.history['val_loss']\r\n",
        "\r\n",
        "epochs_range = range(epochsRound)\r\n",
        "\r\n",
        "plt.figure(figsize=(8, 8))\r\n",
        "plt.subplot(1, 2, 1)\r\n",
        "plt.plot(epochs_range, acc, label='Training Accuracy')\r\n",
        "plt.plot(epochs_range, val_acc, label='Validation Accuracy')\r\n",
        "plt.legend(loc='lower right')\r\n",
        "plt.title('Training and Validation Accuracy')\r\n",
        "\r\n",
        "plt.subplot(1, 2, 2)\r\n",
        "plt.plot(epochs_range, loss, label='Training Loss')\r\n",
        "plt.plot(epochs_range, val_loss, label='Validation Loss')\r\n",
        "plt.legend(loc='upper right')\r\n",
        "plt.title('Training and Validation Loss')\r\n",
        "plt.show()"
      ],
      "outputs": [],
      "metadata": {
        "id": "Gpb3V8qE6iGR"
      }
    },
    {
      "cell_type": "markdown",
      "source": [
        "# References  \n",
        "\n",
        "1. EfficientNetV2:  \n",
        "https://github.com/google/automl/tree/master/efficientnetv2  \n",
        "2. Transfer learning and fine-tuning  \n",
        "https://www.tensorflow.org/tutorials/images/transfer_learning?hl=zh-tw  \n"
      ],
      "metadata": {
        "id": "01BVRXZ_879x"
      }
    }
  ]
}